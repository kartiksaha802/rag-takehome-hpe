{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbcad8c6",
   "metadata": {},
   "source": [
    "# Minimal Local RAG: Llama 3.2 1B Implementation\n",
    "**Context:** Technical Interview Assignment | **Infrastructure:** Local CPU Optimized\n",
    "\n",
    "## 1. Executive Summary\n",
    "This notebook implements a self-contained **Retrieval-Augmented Generation (RAG)** system designed to run efficiently on standard local hardware. By utilizing **Quantized Small Language Models (SLMs)** and a file-based vector store, we achieve low-latency semantic search and generation without requiring GPU resources or external cloud dependencies.\n",
    "\n",
    "## 2. Architecture: Local vs. Production\n",
    "To adhere to the assignment constraints while demonstrating readiness for enterprise scale, the system is designed with a clear separation between the current \"Minimal\" implementation and the standard \"Production\" architecture:\n",
    "\n",
    "* **Inference Strategy:**\n",
    "    * *Current:* We use **Llama-3.2-1B (Int4 Quantized)** running on `llama.cpp` to optimize for local CPU memory (<1GB).\n",
    "    * *Production:* This would scale to larger enterprise models (e.g., Llama 3 70B) hosted on **vLLM** or **Triton Inference Server** with GPU acceleration.\n",
    "\n",
    "* **Vector Storage:**\n",
    "    * *Current:* **ChromaDB** is configured as a local persistent client for simplicity and zero-setup.\n",
    "    * *Production:* Data would migrate to a distributed vector database like **Milvus** or **Weaviate** to handle millions of vectors with high availability.\n",
    "\n",
    "* **Orchestration & Ingestion:**\n",
    "    * *Current:* A linear Python pipeline handles document processing.\n",
    "    * *Production:* Automated workflows using **Kubeflow** or **Airflow** DAGs would manage continuous data ingestion and retraining pipelines.\n",
    "\n",
    "---\n",
    "### Phase 1: System Initialization\n",
    "*Objective: Configure the runtime environment. The system automates dependency checks and provisions the model artifact from the registry if not present locally.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6e6a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifact found. Ready for inference.\n",
      "RAG Engine Core loaded successfully.\n",
      "\n",
      "Booting RAG Subsystems...\n",
      "Initializing Vector Store...\n",
      "Loading Llama 3.2 1B (Quantized)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Suppress llama.cpp warnings about duplicate tokens\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning, module='llama_cpp.llama')\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# Defines the specific model artifact to be used (Quantized Llama 3.2 1B)\n",
    "REPO_ID = \"bartowski/Llama-3.2-1B-Instruct-GGUF\"\n",
    "FILENAME = \"Llama-3.2-1B-Instruct-Q4_K_M.gguf\"\n",
    "\n",
    "# Define paths relative to this notebook\n",
    "# Notebook is in 'notebooks/', so we go up one level ('..') to reach root\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "MODEL_DIR = os.path.join(PROJECT_ROOT, \"models\")\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, FILENAME)\n",
    "DB_PATH = os.path.join(PROJECT_ROOT, \"chroma_db\")\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "\n",
    "# --- 2. AUTOMATED MODEL PROVISIONING ---\n",
    "# Checks for the model artifact. If missing, it downloads it automatically.\n",
    "# In a real HPE environment, this would pull from a secure container registry or Artifactory.\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(f\"\\nModel artifact not found locally. Initiating download from HuggingFace...\")\n",
    "    print(f\"   ‚Ä¢ Repo: {REPO_ID}\")\n",
    "    \n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    try:\n",
    "        hf_hub_download(\n",
    "            repo_id=REPO_ID,\n",
    "            filename=FILENAME,\n",
    "            local_dir=MODEL_DIR,\n",
    "            local_dir_use_symlinks=False\n",
    "        )\n",
    "        print(\"Download complete. Artifact verified.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Critical Error: Failed to download model. {e}\")\n",
    "        raise e\n",
    "else:\n",
    "    print(f\"Model artifact found. Ready for inference.\")\n",
    "\n",
    "# --- 3. MODULE IMPORT SETUP ---\n",
    "# Appends the project root to the system path to allow importing from 'src'\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "try:\n",
    "    from src.rag_engine import RAGSystem, LocalVectorStore, ingest_file\n",
    "    print(\"RAG Engine Core loaded successfully.\")\n",
    "except ImportError as e:\n",
    "    print(\"Error: Could not import 'src.rag_engine'. Verify the 'src' folder exists in project root.\")\n",
    "    raise e\n",
    "\n",
    "# --- 4. RUNTIME INITIALIZATION ---\n",
    "print(\"\\nBooting RAG Subsystems...\")\n",
    "\n",
    "# Initialize Vector Store (ChromaDB)\n",
    "# Persistence ensures we don't need to re-index data on every restart\n",
    "store = LocalVectorStore(persistence_path=DB_PATH)\n",
    "\n",
    "# Initialize Inference Engine (Llama.cpp)\n",
    "# Loads the GGUF model into CPU memory\n",
    "rag = RAGSystem(model_path=MODEL_PATH, vector_store=store)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535b15fe",
   "metadata": {},
   "source": [
    "### Phase 2: Knowledge Ingestion (ETL Simulation)\n",
    "*Objective: Transform unstructured technical documentation into a semantic vector index.*\n",
    "\n",
    "In a large-scale production context, this ingestion process is typically handled by distributed **ETL pipelines** (e.g., using **Apache Airflow**) or dedicated data curation frameworks to manage continuous updates.\n",
    "\n",
    "For this specific assignment, we simulate this workflow using a lightweight **Recursive Chunking Strategy** (500 chars). This approach balances semantic context retention with the memory constraints of local vector search.\n",
    "\n",
    "**Dynamic Multi-Format Ingestion:**\n",
    "The system automatically discovers and processes all supported files (PDF, TXT, MD) in the data directory, providing a flexible and scalable document ingestion pipeline suitable for enterprise environments where new documents are continuously added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f01f05fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning data directory for documents...\n",
      "Found 3 document(s) to process:\n",
      "   ‚Ä¢ 2502019_AI-Governance-Dialogue-Steering-the-Future-of-AI-2025.pdf\n",
      "   ‚Ä¢ deepseek_v3_specs.txt\n",
      "   ‚Ä¢ GEP-June-2025.pdf\n",
      "\n",
      "Starting ingestion pipeline...\n",
      "\n",
      "Processing: 2502019_AI-Governance-Dialogue-Steering-the-Future-of-AI-2025.pdf\n",
      "Extracting text from PDF: c:\\Users\\kartik.saha\\Desktop\\rag-takehome-hpe\\data\\2502019_AI-Governance-Dialogue-Steering-the-Future-of-AI-2025.pdf\n",
      "Extracted 257993 characters from 100 pages.\n",
      "Embedding 574 chunks...\n",
      "Indexed 574 chunks.\n",
      "\n",
      "Processing: deepseek_v3_specs.txt\n",
      "Embedding 4 chunks...\n",
      "Indexed 4 chunks.\n",
      "\n",
      "Processing: GEP-June-2025.pdf\n",
      "Extracting text from PDF: c:\\Users\\kartik.saha\\Desktop\\rag-takehome-hpe\\data\\GEP-June-2025.pdf\n",
      "Extracted 977299 characters from 254 pages.\n",
      "Embedding 2172 chunks...\n",
      "Indexed 2172 chunks.\n",
      "\n",
      "‚úÖ Document ingestion complete!\n"
     ]
    }
   ],
   "source": [
    "# Auto-discover and ingest all supported files in the data directory\n",
    "print(\"Scanning data directory for documents...\")\n",
    "\n",
    "# Supported file extensions\n",
    "SUPPORTED_EXTENSIONS = ['.pdf', '.txt', '.md']\n",
    "\n",
    "# Discover all files\n",
    "data_files = []\n",
    "if os.path.exists(DATA_DIR):\n",
    "    for filename in os.listdir(DATA_DIR):\n",
    "        file_path = os.path.join(DATA_DIR, filename)\n",
    "        if os.path.isfile(file_path):\n",
    "            file_ext = os.path.splitext(filename)[1].lower()\n",
    "            if file_ext in SUPPORTED_EXTENSIONS:\n",
    "                data_files.append(file_path)\n",
    "\n",
    "print(f\"Found {len(data_files)} document(s) to process:\")\n",
    "for file_path in data_files:\n",
    "    print(f\"   ‚Ä¢ {os.path.basename(file_path)}\")\n",
    "\n",
    "# Ingest all discovered files\n",
    "print(\"\\nStarting ingestion pipeline...\")\n",
    "for file_path in data_files:\n",
    "    print(f\"\\nProcessing: {os.path.basename(file_path)}\")\n",
    "    try:\n",
    "        ingest_file(file_path, store)\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Error processing {os.path.basename(file_path)}: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Document ingestion complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca2802b",
   "metadata": {},
   "source": [
    "### Phase 3: Telemetry & Observability\n",
    "*Objective: Validate response latency and retrieval accuracy (Grounding).*\n",
    "\n",
    "In a distributed production environment, observability is typically managed via APM tools like **Prometheus**, **Grafana**, or **OpenTelemetry** traces to monitor system health and **Service Level Indicators (SLIs)**.\n",
    "\n",
    "For this local implementation, we inject a lightweight telemetry wrapper directly into the execution path. This provides immediate, real-time visibility into:\n",
    "1.  **Retrieval Latency:** The specific time cost of the vector search operation.\n",
    "2.  **Semantic Distance:** The L2 distance scores (lower is better), allowing us to audit the \"Grounding\" of the retrieved context and detect potential hallucinations.\n",
    "3.  **Generation Latency:** The CPU time required for the quantized model to tokenize and generate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6c71090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì User Query: With global compute access becoming a strategic factor in 2025, how do economic trends influence scaling RAG systems for enterprises?\n",
      "--------------------------------------------------\n",
      "üîç Retrieval Phase (0.1343s):\n",
      "   [Chunk 1] Distance Score: 0.8351 | Content: tion that diminishing returns may have been reache...\n",
      "   [Chunk 2] Distance Score: 0.9962 | Content: ation technologies.‚Äù\n",
      "Mr Kon√© warned that the lack ...\n",
      "   [Chunk 3] Distance Score: 1.0195 | Content: in 2000-04 to less \n",
      "than 30 percent in 2019-23. \n",
      "T...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kartik.saha\\Desktop\\rag-takehome-hpe\\venv\\Lib\\site-packages\\llama_cpp\\llama.py:1242: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "ü§ñ Model Response (1.45s):\n",
      "I do not know.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Update your query function to include 'debug=True'\n",
    "def query_with_telemetry(rag_system, user_query):\n",
    "    print(f\"‚ùì User Query: {user_query}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # 1. Measure Retrieval Time\n",
    "    start_time = time.time()\n",
    "    results = rag_system.vector_store.collection.query(\n",
    "        query_embeddings=rag_system.vector_store.embedder.encode([user_query]).tolist(),\n",
    "        n_results=3\n",
    "    )\n",
    "    retrieval_time = time.time() - start_time\n",
    "    \n",
    "    # 2. Show the \"Why\": Print Similarity Scores (Distance)\n",
    "    # Chroma returns 'distances'. Lower is better for L2, Higher is better for Cosine.\n",
    "    # Assuming default (L2 squared), smaller = closer.\n",
    "    print(f\"üîç Retrieval Phase ({retrieval_time:.4f}s):\")\n",
    "    for i, (doc, dist) in enumerate(zip(results['documents'][0], results['distances'][0])):\n",
    "        print(f\"   [Chunk {i+1}] Distance Score: {dist:.4f} | Content: {doc[:50]}...\")\n",
    "        \n",
    "    # 3. Measure Generation Time\n",
    "    start_gen = time.time()\n",
    "    answer = rag_system.query(user_query) # Your existing function\n",
    "    gen_time = time.time() - start_gen\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"ü§ñ Model Response ({gen_time:.2f}s):\")\n",
    "    print(answer)\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Run it\n",
    "query_with_telemetry(rag, \"With global compute access becoming a strategic factor in 2025, how do economic trends influence scaling RAG systems for enterprises?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa7c7b9",
   "metadata": {},
   "source": [
    "### Phase 4: Interactive Validation (Acceptance Testing)\n",
    "*Objective: Execute live queries to verify retrieval precision and generation quality against the ingested knowledge base.*\n",
    "\n",
    "We perform two distinct types of validation tests to ensure the system meets the expected **Quality of Service (QoS)**:\n",
    "* **Test Case 1 (Factual Recall):** Validates the system's ability to retrieve precise quantitative data (e.g., hardware specifications) which the base model would not know.\n",
    "* **Test Case 2 (Conceptual Synthesis):** Tests the system's capacity to retrieve multiple chunks and synthesize a technical explanation of a complex architectural component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9538165c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùì User Query: How is AI governance evolving in 2025 with the rise of AI agents, and what implications does it have for building RAG systems responsibly?\n",
      "--------------------------------------------------\n",
      "üîç Retrieval Phase (0.0132s):\n",
      "   [Chunk 1] Distance Score: 0.4657 | Content: and Trends in AI Governance  ........................\n",
      "   [Chunk 2] Distance Score: 0.4673 | Content: \n",
      "coordination, technical standards, infrastructure...\n",
      "   [Chunk 3] Distance Score: 0.4810 | Content: . (2025, April 17), Fn. 1\n",
      "5 Gabriel, I., Manzini, ...\n",
      "--------------------------------------------------\n",
      "ü§ñ Model Response (2.97s):\n",
      "The text does not provide detailed information about AI governance evolution, but it mentions that the field of AI has grown to encompass chat-style tools and AI agents. It suggests that AI agents are becoming more prominent, but the specific implications for building RAG (Robust Architectural Governance) systems are not detailed.\n",
      "==================================================\n",
      "‚ùì User Query: Explain the Multi-Head Latent Attention.\n",
      "--------------------------------------------------\n",
      "üîç Retrieval Phase (0.0106s):\n",
      "   [Chunk 1] Distance Score: 1.1054 | Content: DeepSeek-V3 Technical Report\n",
      "Date: December 2024\n",
      "D...\n",
      "   [Chunk 2] Distance Score: 1.1054 | Content: DeepSeek-V3 Technical Report\n",
      "Date: December 2024\n",
      "D...\n",
      "   [Chunk 3] Distance Score: 1.1054 | Content: DeepSeek-V3 Technical Report\n",
      "Date: December 2024\n",
      "D...\n",
      "--------------------------------------------------\n",
      "ü§ñ Model Response (2.05s):\n",
      "Multi-Head Latent Attention is a key architecture feature in DeepSeek-V3, which improves inference efficiency by compressing the Key-Va mechanism.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Factual Query\n",
    "query_with_telemetry(rag, \"How is AI governance evolving in 2025 with the rise of AI agents, and what implications does it have for building RAG systems responsibly?\")\n",
    "\n",
    "# Test 2: Architecture Query\n",
    "query_with_telemetry(rag, \"Explain the Multi-Head Latent Attention.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f1654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
